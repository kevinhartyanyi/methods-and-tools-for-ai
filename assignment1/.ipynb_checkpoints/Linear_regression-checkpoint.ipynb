{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2pKobZd0KxwP"
   },
   "source": [
    "\n",
    "**Created by:**\n",
    "\n",
    "__[Viktor Varga](https://github.com/vvarga90)__, __Gulyás János Adrián__  \n",
    "**Edited by Márton Véges**\n",
    "\n",
    "<br>\n",
    "\n",
    "<img src=\"https://docs.google.com/uc?export=download&id=1WzgXsCoz8O-NeBlJTbuLPC1iIFDmgYt1\" style=\"display:inline-block\">\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UPNGQgO6aXtZ"
   },
   "source": [
    "# Linear regression using gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EaVq4W9qOIIv"
   },
   "source": [
    "Source of the databases at: http://www.dcc.fc.up.pt/~ltorgo/Regression/DataSets.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9xF9psXQKgN7"
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "eugRKr5_Kf7c"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import urllib.request\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UxnhbGQ-Jzxx"
   },
   "source": [
    "## Getting the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b8DWij2kFZRu"
   },
   "source": [
    "Loading in the data (\"Wine Quality Data Set\" from the UCI ML repository).\n",
    "\n",
    "**Exercise:** Let's rate the quality of wines on a scale from 1 to 10 based on the wine's chemical composition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "Iuxqk43jFPCG"
   },
   "outputs": [],
   "source": [
    "url = 'https://raw.githubusercontent.com/vvarga90/dlmatek_public_files/master/winequality-red.csv'\n",
    "\n",
    "ftpstream = urllib.request.urlopen(url)\n",
    "content_reg = ftpstream.read().decode('utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M_XRgr1mJFhs"
   },
   "source": [
    "Processing data: from text file to numpy array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 112
    },
    "id": "dPMWRt5qHfDy",
    "outputId": "2735087f-171e-4e21-fccc-1a20d3c2c59c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data attributes:\n",
      "['\"fixed acidity\"', '\"volatile acidity\"', '\"citric acid\"', '\"residual sugar\"', '\"chlorides\"', '\"free sulfur dioxide\"', '\"total sulfur dioxide\"', '\"density\"', '\"pH\"', '\"sulphates\"', '\"alcohol\"', '\"quality\"']\n",
      "(1000, 11)\n",
      "(1000,)\n"
     ]
    }
   ],
   "source": [
    "lines = content_reg.split('\\n')\n",
    "words = [line.split(';') for line in lines]\n",
    "attr_names = words[0]\n",
    "data = words[1:1001]\n",
    "data = [[float(item) for item in rec] for rec in data]\n",
    "data = np.array(data, dtype=np.float32)\n",
    "\n",
    "features = data[:,:-1]\n",
    "labels = data[:,-1]\n",
    "\n",
    "print(\"Data attributes:\")\n",
    "print(attr_names)\n",
    "print(features.shape)\n",
    "print(labels.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 129
    },
    "id": "MAAMvhNVZHsR",
    "outputId": "0048e4a5-f19c-499c-b47b-24ced27a9545"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4.6     0.12    0.      1.2     0.012   1.      6.      0.99064 2.74\n",
      " 0.33    8.4     3.     ]\n",
      "[ 15.9      1.33     1.      15.5      0.611   68.     165.       1.0032\n",
      "   3.9      2.      14.9      8.    ]\n",
      "[ 8.728898    0.52829     0.29458055  2.5794      0.09037506 15.171\n",
      " 48.328       0.9973502   3.2990983   0.66852003 10.240709    5.594     ]\n"
     ]
    }
   ],
   "source": [
    "# what is the range? what is the min/max? what is the mean?\n",
    "print(np.amin(data,axis=0))\n",
    "print(np.amax(data,axis=0))\n",
    "print(np.mean(data,axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "id": "MrFEaDTRb8HE",
    "outputId": "0c181830-b045-4886-b72a-59e22fc13dce"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Its also useful to plot histograms of the features\n",
    "plt.figure()\n",
    "plt.hist(data[:, 0], bins=50);  # Jupyter tip: add a ';' at the end, to suppress the text output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_H7B_EggKKrE"
   },
   "source": [
    "## Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "munYcZbIksAL"
   },
   "source": [
    "As a reminder we would like to find the optimal parameter $\\theta$ for the function\n",
    "$$f_{\\theta}(x)=\\theta_0+\\theta x$$\n",
    "such that it minimizes the squared error:\n",
    "$$L(\\mathcal{D},\\theta)=\\frac{1}{N}\\sum_{n=1}^N(y_n-f_{\\theta}(x_n))^2.$$\n",
    "\n",
    "On the lecture we learned about the normal equation to solve this:\n",
    "$$\\theta=(X^TX)^{-1}X^Ty,$$\n",
    "where each row of $X$ contains the features of one datapoint from the dataset. Turns out this is a really bad approach to solve this problem due to numerical instabilities (see the exercise at the end of the notebook) and also very slow if you have many features. Instead, we are going to use gradient descent, to solve the problem. First note that (ignoring $\\theta_0$):\n",
    "$$L(\\mathcal{D},\\theta)=\\frac{1}{N}\\sum_{n=1}^N(y_n-f_{\\theta}(x_n))^2=\\frac{1}{N}\\sum_{n=1}^N(y_n-\\theta x_n)^2.$$\n",
    "This is a quadratic function in $\\theta$, we can plot it:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KeTUVn2eWoHM"
   },
   "source": [
    "![alt](https://drive.google.com/uc?export=download&id=1kAr_3NqddrbZua1Z5EOgzPnJlQphpKX6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6DDTpewTWDkJ"
   },
   "source": [
    "The algorithm of gradient descent:\n",
    "1. Initialize $\\theta^{(0)}$ to zero (or set randomly)\n",
    "2. Update $\\theta$:  \n",
    "$\\theta^{(t)}=\\theta^{(t-1)}−\\alpha\\nabla_{\\theta} L(\\mathcal D, \\theta)$\n",
    "3. Iterate until it converges, or for a certain number of iterations.\n",
    "\n",
    "In our case $\\nabla_{\\theta} L(\\mathcal D, \\theta)=\\frac{1}{N}\\sum_{n=1}^N-2x_n(y_n-\\theta x_n)=-\\frac{2}{N}X^T(y-X\\theta)$. \n",
    "\n",
    "See below the implementation of the linear regression class. `fit_intercept` is `True` if we want to use the $\\theta_0$ constant parameter. The trick to add back $\\theta_0$ to our formula:\n",
    "$$x_n'=(1,x_n),\\  \\theta'=\\begin{pmatrix}\\theta_0 \\\\ \\theta \\end{pmatrix}$$\n",
    "\n",
    "$$L(\\mathcal D, \\theta)=\\frac{1}{N}\\sum_{n=1}^N(y_n-\\theta_0-\\theta x_n)^2=\\frac{1}{N}\\sum_{n=1}^N(y_n-\\theta' x_n')^2$$\n",
    "In vectorized form:\n",
    "$$X'=\\left[\\begin{array}{c|c}\n",
    "\\begin{matrix}1 \\\\ \\vdots \\\\ 1 \\end{matrix} & X \n",
    "\\end{array}\n",
    "\\right]$$\n",
    "This trick lets us handle $\\theta_0$ as part of $\\theta$, simplifying our code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "UIe5x8koksAj"
   },
   "outputs": [],
   "source": [
    "class LinearRegression:\n",
    "    def __init__(self, lr=0.01, num_iter=10000, fit_intercept=True):\n",
    "        self.lr = lr\n",
    "        self.num_iter = num_iter\n",
    "        self.fit_intercept = fit_intercept\n",
    "        self.train_theta = []\n",
    "        #self.eps = 0.00000001\n",
    "    \n",
    "    def __add_intercept(self, X):\n",
    "        # add a column of 1s\n",
    "        intercept = np.ones((X.shape[0], 1))\n",
    "        return np.concatenate((intercept, X), axis=1)\n",
    "    \n",
    "    def __loss(self, h, y):\n",
    "        return np.mean(np.square(h - y))\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        if self.fit_intercept:\n",
    "            X = self.__add_intercept(X)\n",
    "        \n",
    "        # weights initialization\n",
    "        self.theta = np.zeros(X.shape[1])\n",
    "        \n",
    "        loss_vals = []\n",
    "        t_theta = []\n",
    "        for i in range(self.num_iter):\n",
    "            h = np.dot(X, self.theta)\n",
    "            gradient = 2*np.dot(X.T, h - y) / y.size\n",
    "            self.theta -= self.lr * gradient\n",
    "            \n",
    "            t_theta.append(np.copy(self.theta))\n",
    "            \n",
    "            h = np.dot(X, self.theta)\n",
    "            loss = self.__loss(h, y)\n",
    "            loss_vals.append(loss)\n",
    "        \n",
    "        return loss_vals, t_theta\n",
    "    \n",
    "    def predict(self, X):\n",
    "        if self.fit_intercept:\n",
    "            X = self.__add_intercept(X)\n",
    "    \n",
    "        return np.dot(X, self.theta)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 7.4   0.7   0.   ...  3.51  0.56  9.4 ]\n",
      " [ 7.8   0.88  0.   ...  3.2   0.68  9.8 ]\n",
      " [ 7.8   0.76  0.04 ...  3.26  0.65  9.8 ]\n",
      " ...\n",
      " [ 7.2   0.34  0.32 ...  3.32  0.79 11.1 ]\n",
      " [10.7   0.35  0.53 ...  3.15  0.65 11.  ]\n",
      " [ 8.7   0.69  0.31 ...  3.48  0.74 11.6 ]]\n",
      "[[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'X' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-8cc06c95b1ab>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mintercept\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mintercept\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'X' is not defined"
     ]
    }
   ],
   "source": [
    "print(X_train)\n",
    "intercept = np.ones((X_train.shape[0], 1))\n",
    "print(intercept)\n",
    "\n",
    "print(np.concatenate((intercept, X), axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NYO1QIceKdEe"
   },
   "source": [
    "## Preprocessing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1YocW5c_Ku4W"
   },
   "source": [
    "Selecting X, Y, separating the data into a train and a test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 92
    },
    "id": "6ATDtyb6K-dK",
    "outputId": "8a9d9bbf-4435-4533-c0cb-796a6315f15f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(500, 11)\n",
      "(500,)\n",
      "(500, 11)\n",
      "(500,)\n"
     ]
    }
   ],
   "source": [
    "X_train = features[:500]\n",
    "y_train = labels[:500]\n",
    "X_test = features[500:]\n",
    "y_test = labels[500:]\n",
    "\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y_tZc4QSbeev"
   },
   "source": [
    "Feature scaling methods: one for calculating the average and the deviation of each feature, which will be used for normalizing the input, and another one that normalizes an input based on the already calculated average and deviation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "L2pQ3lkUbhcv"
   },
   "outputs": [],
   "source": [
    "def std_normalization(data):\n",
    "    # params: data: ndarray(n_samples, n_features)\n",
    "    data = np.asarray(data, dtype=np.float64)\n",
    "    eps = 0.0000001\n",
    "    x_mean = np.mean(data, axis=0)\n",
    "    x_std = np.std(data, axis=0)\n",
    "    x_norm = (data-x_mean) / (x_std + eps)\n",
    "    \n",
    "    return x_norm, x_mean, x_std\n",
    "  \n",
    "def std_normalize_with_given_meanstd(data, x_mean, x_std):\n",
    "    eps = 0.0000001\n",
    "    x_norm = (data-x_mean) / (x_std + eps)\n",
    "    return x_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "Hg6suT-0lwb7"
   },
   "outputs": [],
   "source": [
    "X_train_n, x_mean, x_std = std_normalization(X_train)\n",
    "X_test_n = std_normalize_with_given_meanstd(X_test, x_mean, x_std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "In-I8wdxlwca"
   },
   "source": [
    "## Training and evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 302
    },
    "id": "9bZ5AIn0lwci",
    "outputId": "b2c15c1f-ee2a-40c3-f48c-e2a461989023"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training mean absolute error: 0.5001771395641166\n",
      "Test mean absolute error: 0.5162361807381445\n"
     ]
    }
   ],
   "source": [
    "# if features are not scaled to same magnitude higher learning rate causes big fluctuations\n",
    "# model = LinearRegression(lr=0.00001, num_iter=50000)   \n",
    "\n",
    "model = LinearRegression(lr=0.001, num_iter=50000)\n",
    "\n",
    "loss, t_theta = model.fit(X_train_n, y_train)\n",
    "\n",
    "plt.plot(loss)\n",
    "plt.show();\n",
    "\n",
    "preds_train = model.predict(X_train_n)\n",
    "print(\"Training mean absolute error: \" + str(np.abs(preds_train - y_train).mean()))\n",
    "preds_test = model.predict(X_test_n)\n",
    "print(\"Test mean absolute error: \" + str(np.abs(preds_test - y_test).mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_theta= np.dot(np.dot(np.linalg.inv(np.dot(X_train_n.T, X_train)),X_train.T),y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 5.588     ,  0.15146245, -0.17987487, -0.1207094 ,  0.01039456,\n",
       "       -0.06566937,  0.03065106, -0.15662395,  0.00917041, -0.01475676,\n",
       "        0.10756634,  0.30837123])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 3.93112763e+02, -5.59260961e+02, -1.50251876e+03,  7.48843417e-01,\n",
       "        2.98219262e+03, -3.61247185e+01,  2.05272089e+01, -1.11219848e+05,\n",
       "        2.27590211e+03, -1.44202456e+02,  3.29684088e+01])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training mean absolute error: 0.5001771395641166\n",
      "Test mean absolute error: 0.5162361807381445\n",
      "Training mean absolute error: 85822.76436149086\n",
      "Test mean absolute error: 92641.25228101997\n"
     ]
    }
   ],
   "source": [
    "preds_train = model.predict(X_train_n)\n",
    "print(\"Training mean absolute error: \" + str(np.abs(preds_train - y_train).mean()))\n",
    "preds_test = model.predict(X_test_n)\n",
    "print(\"Test mean absolute error: \" + str(np.abs(preds_test - y_test).mean()))\n",
    "\n",
    "\n",
    "print(\"Training mean absolute error: \" + str(np.abs(np.dot(X_train_n, n_theta) - y_train).mean()))\n",
    "preds_test = model.predict(X_test_n)\n",
    "print(\"Test mean absolute error: \" + str(np.abs(np.dot(X_test_n, n_theta) - y_test).mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "theta_means = [x.mean() for x in t_theta]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f027fac3fa0>]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.plot(theta_means)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.74998298, 0.72978515, 0.72978515, 0.54252213, 0.51321053,\n",
       "       0.51391834, 0.55576404, 0.48786905, 0.36983303, 0.54825347,\n",
       "       0.77988776, 0.36983303, 0.28360459, 0.78675706, 0.78675706,\n",
       "       0.0747017 , 0.46569878, 0.56245544, 0.67373534, 0.01320689,\n",
       "       0.20745125, 0.34775734, 0.0408415 , 0.14861931, 0.61382622,\n",
       "       0.67373534, 0.18331795, 0.80127641, 0.42834654, 0.19548684,\n",
       "       0.96948933, 0.96948933, 0.93322749, 0.23246237, 0.19548684,\n",
       "       0.42834654, 0.36076743, 0.29860022, 0.47617875, 0.11660472,\n",
       "       0.51320177, 0.01600046, 0.24492273, 0.05468622, 0.54336529,\n",
       "       0.27907373, 0.08209792, 0.74681384, 0.52829265, 0.19140592,\n",
       "       0.19838631, 0.01713484, 0.25258407, 0.25258407, 0.72268985,\n",
       "       0.00477935, 0.00918075, 0.41495914, 0.72268985, 0.94200051,\n",
       "       0.94200051, 0.21555005, 0.03356557, 0.37249682, 0.03356557,\n",
       "       0.26735547, 0.22509665, 0.0963786 , 0.15938867, 0.21947228,\n",
       "       0.68704222, 0.68704222, 0.52120072, 0.90497004, 0.83656028,\n",
       "       0.59188724, 0.73514358, 0.24463838, 0.7977233 , 0.32096616,\n",
       "       0.63332705, 0.32096616, 0.21321298, 0.19914152, 0.00728622,\n",
       "       0.22756974, 0.26093625, 0.5337951 , 0.30422107, 0.3861141 ,\n",
       "       0.18548256, 0.3861141 , 0.78877942, 0.64088203, 0.47413396,\n",
       "       0.33415608, 0.44666019, 0.39009238, 0.41259712, 0.86295935,\n",
       "       0.41120126, 0.69081669, 0.44836515, 0.24527246, 0.24527246,\n",
       "       0.09430018, 0.92557934, 0.87155801, 0.06843417, 0.07838778,\n",
       "       0.52260444, 0.37284619, 0.45540892, 0.45540892, 0.24743493,\n",
       "       0.24743493, 0.62775433, 0.0659454 , 0.62775433, 0.74627673,\n",
       "       0.16964153, 0.38887007, 0.36753131, 0.3701196 , 0.36437707,\n",
       "       0.25686933, 0.41380814, 0.25140367, 0.41380814, 0.25140367,\n",
       "       0.41380814, 0.53693991, 0.91936334, 0.48721055, 0.64305831,\n",
       "       0.24978636, 0.30949373, 0.36271463, 0.50867379, 0.64305831,\n",
       "       0.36760301, 0.36760301, 0.28106423, 0.49393889, 0.15756799,\n",
       "       0.90797047, 0.5505815 , 0.66719218, 0.19874232, 0.76858316,\n",
       "       0.19874232, 0.60164822, 0.28571943, 0.72314706, 0.28571943,\n",
       "       0.26438681, 0.83886469, 0.26438681, 0.32418955, 0.2254826 ,\n",
       "       0.79825095, 0.571337  , 0.31660235, 0.50215373, 0.7902479 ,\n",
       "       0.50390046, 0.7902479 , 0.09564059, 0.44139691, 0.38623519,\n",
       "       0.60902698, 0.10974485, 0.02235923, 0.10205429, 0.1084413 ,\n",
       "       0.32270103, 0.60520676, 0.60520676, 0.29861252, 0.19233767,\n",
       "       0.98555446, 0.60520676, 0.61560862, 0.01072124, 0.28334432,\n",
       "       0.69363681, 0.09830777, 0.13805344, 0.00958588, 0.04618951,\n",
       "       0.34792572, 0.27862913, 0.21472419, 0.71433534, 0.27862913,\n",
       "       0.60384798, 0.5305342 , 0.26794085, 0.5305342 , 0.15887657,\n",
       "       0.68096399, 0.75094537, 0.73937548, 0.18670272, 0.48241067,\n",
       "       0.48241067, 0.09831807, 0.01397104, 0.83746284, 0.16389523,\n",
       "       0.17550673, 0.29024858, 0.01339425, 0.01339425, 0.63998484,\n",
       "       0.19785468, 0.40229385, 0.08253865, 0.07279741, 0.33493173,\n",
       "       0.50261522, 0.3276343 , 0.50474071, 0.77752251, 0.34129775,\n",
       "       0.49218211, 0.50474071, 0.35498825, 0.35498825, 0.27269892,\n",
       "       0.35498825, 0.65119958, 0.60597274, 0.25300934, 0.25300934,\n",
       "       0.29238656, 0.16732414, 0.27752286, 0.3810715 , 0.27752286,\n",
       "       0.65668318, 0.64968188, 0.2129364 , 0.05245296, 0.83800165,\n",
       "       0.32290394, 0.83800165, 0.16895523, 0.05329184, 0.4962666 ,\n",
       "       0.50954614, 0.11495777, 0.91341514, 0.63690013, 0.81119555,\n",
       "       0.30784904, 0.78233417, 0.44406405, 0.1764482 , 0.44406405,\n",
       "       0.26525577, 0.48616677, 0.48616677, 0.35947028, 0.35947028,\n",
       "       0.06328067, 0.68346644, 0.01078209, 0.80303414, 0.93162395,\n",
       "       0.61098794, 0.58900415, 0.47338359, 0.89600019, 0.08193321,\n",
       "       0.08193321, 0.00971441, 0.69219515, 0.77096501, 0.57139798,\n",
       "       0.37564097, 0.24002292, 0.25891282, 0.24002292, 0.45492993,\n",
       "       0.30159461, 0.79297925, 0.21689473, 0.36375091, 0.0704357 ,\n",
       "       0.62724172, 0.15238132, 0.29751899, 0.38218779, 0.19092643,\n",
       "       0.44547794, 0.44547794, 0.66606459, 0.77511046, 0.82645231,\n",
       "       0.77511046, 0.14457193, 0.14457193, 0.2571724 , 0.15987894,\n",
       "       0.59139043, 0.48983551, 0.47023379, 0.55341132, 0.09471857,\n",
       "       0.19325246, 0.21159811, 0.29746124, 0.29746124, 0.61110618,\n",
       "       0.29746124, 0.29729778, 0.52185202, 0.52185202, 0.42890077,\n",
       "       0.07307186, 0.07307186, 0.90292417, 0.07307186, 0.75900944,\n",
       "       0.72582768, 0.03297656, 0.08639129, 0.68119139, 0.15140257,\n",
       "       0.08639129, 0.12507991, 0.07432971, 0.0374517 , 0.03297656,\n",
       "       0.1699871 , 0.02710523, 0.70943894, 0.66102261, 0.54876401,\n",
       "       0.5472832 , 0.02710523, 0.57052539, 0.11614665, 0.63745124,\n",
       "       0.02646409, 0.42786705, 0.11614665, 0.57052539, 0.65122676,\n",
       "       0.39618766, 0.56357804, 0.17749385, 0.05678451, 0.67687291,\n",
       "       0.13153179, 0.29377816, 0.13153179, 0.81084773, 0.24772623,\n",
       "       0.44855608, 0.24772623, 0.44855608, 0.12346888, 0.62077598,\n",
       "       0.0073636 , 0.24400202, 0.16541915, 0.69626035, 0.18901519,\n",
       "       0.48889542, 0.52145037, 0.16541915, 0.46197548, 0.21411748,\n",
       "       0.35927752, 0.0546036 , 0.28853092, 0.92234938, 0.0546036 ,\n",
       "       0.28853092, 0.35927752, 0.76837642, 0.09213901, 0.49085436,\n",
       "       0.7119543 , 0.5022596 , 0.63064276, 0.5022596 , 0.7119543 ,\n",
       "       0.25406353, 0.25406353, 0.47514155, 0.98285995, 0.40006705,\n",
       "       0.33684854, 0.60983697, 0.76436819, 0.49202549, 0.48121962,\n",
       "       0.35443882, 0.35443882, 0.35443882, 0.48121962, 0.76496218,\n",
       "       0.38278812, 0.26255384, 0.95545178, 0.1218374 , 0.07017919,\n",
       "       0.60925889, 0.14198624, 0.36539882, 0.44159367, 0.26815498,\n",
       "       0.14198624, 0.11367996, 0.63902009, 0.1470122 , 0.35894747,\n",
       "       0.61013156, 0.41960091, 0.41960091, 0.67933804, 0.97423502,\n",
       "       0.48695286, 0.24236742, 0.24236742, 0.1648134 , 0.05604267,\n",
       "       0.38304397, 0.42753353, 0.05604267, 0.07656182, 0.66781662,\n",
       "       0.28371007, 0.55004816, 0.16182656, 0.55004816, 0.30185515,\n",
       "       0.60531561, 0.30185515, 0.31093165, 0.40864438, 0.73244039,\n",
       "       0.73244039, 0.95119275, 0.33122525])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_error = np.abs(preds_test - y_test)\n",
    "less_one = test_error[test_error < 1]\n",
    "less_point_one = test_error[test_error < 0.1]\n",
    "less_one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.hist(less_one, bins=50);\n",
    "plt.hist(less_point_one, bins=50);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "k6VtWM8DT-8W"
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-20-858d66e85edf>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-20-858d66e85edf>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    Calculate how many percent of the test set has an error less than 1/ 0.1\u001b[0m\n\u001b[0m              ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# exercise: print the hsitogram of errors;\n",
    "Calculate how many percent of the test set has an error less than 1/ 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "36dNt0CjMWN9"
   },
   "source": [
    "**Conclusion:** The trained model could predict the wines' quality rating on a scale from 1 to 10 with an absolute error of around 0.5. Considering that most of the wines in the database have a rating between 3 and 8, this is not exactly a remarkable result. Perhaps the wine that's proclaimed to be high quality is the same as a cheaper variant, and calling either wine higher quality is just plain snobbism? Or perhaps a linear function based on the chemical compound is not enough to approximate the quality sticker given by professionals, and we require a more complex model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-JDt0MGaItgs"
   },
   "source": [
    "Linear regression is already implemented in multiple python libraries. One implementation from the sklearn package:\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_o-_D1coZneR"
   },
   "source": [
    "## Exercises\n",
    "\n",
    "**1.** Prove the following vectorized form we used in gradient descent:\n",
    "$$\\nabla_{\\theta} L(\\mathcal D, \\theta)=\\frac{1}{N}\\sum_{n=1}^N-2x_n(y_n-\\theta x_n)=-\\frac{2}{N}X^T(y-X\\theta)$$\n",
    "\n",
    "**2.** Using the trained network inspect its results:  \n",
    "- Print out how `theta` changes during training. Set the learning rate to a low and a high value and see what happens.\n",
    "- Train the linear regression. Plot the histogram of prediction errors.\n",
    "- Calculate the percentage of predictions where the error is less than 1 or less than 0.1\n",
    "\n",
    "\n",
    "**3.** Solve the linear regression problem with the normal equation (see above). Compare it to our gradient descent solution:  \n",
    "- Which one has the lower train/test error?\n",
    "- Which one has the lower loss (the $L(\\mathcal D,\\theta)$ value)?\n",
    "\n",
    "*(For those who did numerical analysis)* We are inverting the matrix $X^TX$, which can be numerically unstable. Calculate the condition number of $X^TX$. What does it mean for the stability of our normal equation? Recall that\n",
    "$$\\kappa(M)=\\frac{\\sigma_{max}(M)}{\\sigma_{min}(M)},$$\n",
    "where $\\sigma_{max}(M)$ and $\\sigma_{max}(M)$ are the maximal and minimal singular values of $M$.\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Linear regression.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
