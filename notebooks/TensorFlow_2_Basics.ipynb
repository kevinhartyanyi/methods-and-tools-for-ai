{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TensorFlow 2 - Basics.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qWoKervshKI8"
      },
      "source": [
        "# Introduction to TensorFlow\n",
        "In this notebook we will go through the basics in TensorFlow, build a simple multilayer perceptron and try out the TensorBoard visualization utility."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QYP0GTRTl1fd"
      },
      "source": [
        "## Basics\n",
        "Here we will see how to write a simple Hello, World! application in TensorFlow and  learn about the basics.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7sDc11D4imrJ"
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QTuamkoclfuQ"
      },
      "source": [
        "### Tensors\n",
        "\n",
        "\n",
        "The basic building blocks of TensorFlow are **Tensors**. Tensors can hold an array of data, very similar to Numpy's `ndarray`. It also has many of the same attributes as an ndarray, like `dtype`, `shape`, `ndim`.\n",
        "\n",
        "\n",
        "Let's see some examples:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3eG7FnaVzMaq",
        "outputId": "64f9ea34-7cca-49aa-eed4-a881a3141ef2"
      },
      "source": [
        "a = tf.constant(3.14)\n",
        "b = tf.constant(42, dtype=tf.float32)\n",
        "c = tf.constant([[1, 2]])\n",
        "\n",
        "print(\"a =\", a)\n",
        "print(\"b =\", b)\n",
        "print(\"c =\", c)\n",
        "print(\"Shape of c:\", c.shape)\n",
        "print()\n",
        "\n",
        "# You can also create Tensors from ndarrays:\n",
        "print(tf.constant(np.ones((2,2), dtype='float32')))\n",
        "print()\n",
        "\n",
        "# But tf.ones is also available\n",
        "print(tf.ones((2,2), dtype='float32'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "a = tf.Tensor(3.14, shape=(), dtype=float32)\n",
            "b = tf.Tensor(42.0, shape=(), dtype=float32)\n",
            "c = tf.Tensor([[1 2]], shape=(1, 2), dtype=int32)\n",
            "Shape of c: (1, 2)\n",
            "\n",
            "tf.Tensor(\n",
            "[[1. 1.]\n",
            " [1. 1.]], shape=(2, 2), dtype=float32)\n",
            "\n",
            "tf.Tensor(\n",
            "[[1. 1.]\n",
            " [1. 1.]], shape=(2, 2), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LQFdcj8q0SZv"
      },
      "source": [
        "Tensors supports many builtin functions, broadcasting and also indexing - pretty much equivalent to Numpy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "04eFNXCGrSwE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "21db6c26-4534-4902-b513-d65d8f145918"
      },
      "source": [
        "print(a + b)\n",
        "print(tf.add(a, b))  # This is the same as the previous\n",
        "\n",
        "print(a * b)\n",
        "\n",
        "# Broadcasting\n",
        "x = tf.constant(3)\n",
        "mul = tf.multiply(x, c)\n",
        "print(mul)\n",
        "\n",
        "# Indexing:\n",
        "print(c[0, 1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(45.14, shape=(), dtype=float32)\n",
            "tf.Tensor(45.14, shape=(), dtype=float32)\n",
            "tf.Tensor(131.88, shape=(), dtype=float32)\n",
            "tf.Tensor([[3 6]], shape=(1, 2), dtype=int32)\n",
            "tf.Tensor(2, shape=(), dtype=int32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YAlHEDlBqkN_"
      },
      "source": [
        "However, Numpy can execute only on CPU, while Tensor operations are usually executed on the GPU. The `device` attribute tells us where a tensor is stured (CPU or GPU memory):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y94aFnlf4-SN",
        "outputId": "71883ccd-9a4e-43ff-9c40-dfa5f0c6f042"
      },
      "source": [
        "a = tf.constant(3)\n",
        "b = tf.constant(3)\n",
        "result = tf.add(a, b)\n",
        "\n",
        "print(\"a.device:\", a.device)\n",
        "print(\"b.device:\", b.device)\n",
        "print(\"result.device:\", result.device)  # The result is on the GPU, as operations are performed on the GPU"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "a.device: /job:localhost/replica:0/task:0/device:CPU:0\n",
            "b.device: /job:localhost/replica:0/task:0/device:CPU:0\n",
            "result.device: /job:localhost/replica:0/task:0/device:GPU:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CbZfeLND9WEc"
      },
      "source": [
        "### Variables\n",
        "\n",
        "Tensors are immutable objects. Tensorflow provides `Variable` whose value can be changed."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vaYi3S8J9LDm",
        "outputId": "843d80ec-3f8b-4b1c-a27e-1252d19dfbcf"
      },
      "source": [
        "v = tf.Variable(1.)\n",
        "print(v)\n",
        "\n",
        "v.assign(2.)\n",
        "print(v)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=1.0>\n",
            "<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=2.0>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "obAZH_r0BPS9"
      },
      "source": [
        "### Calculating gradients\n",
        "\n",
        "The magic of Tensorflow happens in `GradientTape` - it records performed operations (on a \"tape\"), then the `gradient` function calculates the gradient (\"plays the tape backwards\").\n",
        "\n",
        "For example, the following snippets calculates the derivative of `x^2` at 3:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CY5UfaQJBOln",
        "outputId": "4817328c-fc33-486d-e00f-56e0d119284c"
      },
      "source": [
        "x = tf.Variable(3.)\n",
        "second_order = False\n",
        "with tf.GradientTape() as g:\n",
        "  f = x*x\n",
        "\n",
        "\n",
        "print(g.gradient(f, x))  # differentiate f with regards to x"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(6.0, shape=(), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wD9fWLosbm4M"
      },
      "source": [
        "Note: by default GradientTape records only operations that depends on a `Variable`. If you want to include some other tensors as well, use `GradientTape.watch()`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C_Y94Jwi3X5X"
      },
      "source": [
        "## Building  a Multilayer Perceptron\n",
        "\n",
        "Here will see how to build a simple neural network with TensorFlow using only the low-level API.\n",
        "![](http://cs231n.github.io/assets/nn1/neural_net2.jpeg)  \n",
        "\n",
        "It will classify handwritten digits from MNIST. The network will have two dense layers and a final softmax layer with 10 outputs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yRAnuodw3sSS"
      },
      "source": [
        "# First load the MNIST dataset\n",
        "from tensorflow.keras.datasets import mnist\n",
        "\n",
        "# Load MNIST dataset\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "# Normalize the training and test data\n",
        "x_train = x_train / 255.0\n",
        "x_test = x_test / 255.0\n",
        "\n",
        "# Flatten input images into a vector\n",
        "x_train = x_train.reshape(-1, 28*28).astype('float32')\n",
        "x_test = x_test.reshape(-1, 28*28).astype('float32')\n",
        "\n",
        "# use one-hot encoding for y:\n",
        "y_train = tf.keras.utils.to_categorical(y_train)\n",
        "y_test = tf.keras.utils.to_categorical(y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uqx8Cd0GJCjR"
      },
      "source": [
        "# Parameters\n",
        "learning_rate = 0.01\n",
        "num_steps = 500\n",
        "batch_size = 32\n",
        "display_step = 50  # print loss every 50 iterations\n",
        "\n",
        "# Network Parameters\n",
        "n_hidden_1 = 256 # 1st layer number of neurons\n",
        "n_hidden_2 = 256 # 2nd layer number of neurons\n",
        "n_input = 784 # MNIST data input (img shape: 28*28)\n",
        "n_classes = 10 # MNIST total classes (0-9 digits)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pgbAy7YJJG3o"
      },
      "source": [
        "# Store layer weights & biases\n",
        "params = {\n",
        "  'w1': tf.Variable(tf.random.normal([n_input, n_hidden_1]), name='W1'),\n",
        "  'w2': tf.Variable(tf.random.normal([n_hidden_1, n_hidden_2]), name='W2'),\n",
        "  'w3': tf.Variable(tf.random.normal([n_hidden_2, n_classes]), name='W3'),\n",
        "  'b1': tf.Variable(tf.random.normal([n_hidden_1]), name='b1'),\n",
        "  'b2': tf.Variable(tf.random.normal([n_hidden_2]), name='b2'),\n",
        "  'b3': tf.Variable(tf.random.normal([n_classes]), name='b3')\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MqbxNdULJJLS"
      },
      "source": [
        "# Create the model\n",
        "def neural_net(x):\n",
        "  # A fully connected layer with 256 neurons.\n",
        "  # Remember, a fully connected layer's formula is\n",
        "  #     o = g(x*W+b)\n",
        "  # where g is the activation function, W is weight matrix and b is the bias vector.\n",
        "  \n",
        "  # Equivalent to:\n",
        "  # layer_1_out = Dense(n_hidden_1, activation='relu')(x)\n",
        "  a1 = tf.add(tf.matmul(x, params['w1']), params['b1'])  # shape: (batch_size, n_hidden_1)\n",
        "  layer_1_out = tf.nn.relu(a1)  \n",
        "  \n",
        "  a2 = tf.multiply(tf.matmul(layer_1_out, params['w2']), params['b2'])  # shape: (batch_size, n_hidden_2)\n",
        "  layer_2_out = tf.nn.relu(a2)\n",
        "  \n",
        "  layer_3_out = tf.matmul(layer_2_out, params['w3']) + params['b3']  # shape: (batch_size, n_classes)\n",
        "  return tf.nn.softmax(layer_3_out)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GcEmQQG8GDDQ",
        "outputId": "b04a7a58-66c0-498e-9b5b-170ca0447115"
      },
      "source": [
        "loss_fn = tf.keras.losses.CategoricalCrossentropy()\n",
        "\n",
        "for step in range(num_steps):\n",
        "  # STEPth 32 vectors from x_train/y_train\n",
        "  batch_x = x_train[step*batch_size:(step+1)*batch_size]\n",
        "  batch_y = y_train[step*batch_size:(step+1)*batch_size]\n",
        "\n",
        "  with tf.GradientTape() as tape:\n",
        "    pred = neural_net(batch_x)\n",
        "    loss = loss_fn(batch_y, pred)\n",
        "\n",
        "    # Calculate gradients  \n",
        "    grads = tape.gradient(loss, params)\n",
        "\n",
        "  # Update the weights\n",
        "  for name in params:\n",
        "     #  params[name] -= learning_rate * grads[name]\n",
        "      params[name].assign_sub(learning_rate * grads[name])  # subtract lr*gradient from each weight\n",
        "\n",
        "  if (step+1) % display_step == 0:\n",
        "     # calculate performance on training set\n",
        "     pred = neural_net(x_train)\n",
        "     pred_class = np.argmax(pred, axis=1)\n",
        "     gt_class = np.argmax(y_train, axis=1)\n",
        "\n",
        "     acc = np.mean(gt_class==pred_class)\n",
        "\n",
        "     print(\"Step \" + str(step) + \", Minibatch Loss= \" + \\\n",
        "            \"{:.4f}\".format(loss) + \", Training Accuracy= \" + \\\n",
        "            \"{:.3f}\".format(acc))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Step 49, Minibatch Loss= 12.5923, Training Accuracy= 0.133\n",
            "Step 99, Minibatch Loss= 14.1033, Training Accuracy= 0.168\n",
            "Step 149, Minibatch Loss= 14.1033, Training Accuracy= 0.172\n",
            "Step 199, Minibatch Loss= 13.0960, Training Accuracy= 0.172\n",
            "Step 249, Minibatch Loss= 12.0886, Training Accuracy= 0.181\n",
            "Step 299, Minibatch Loss= 14.6070, Training Accuracy= 0.187\n",
            "Step 349, Minibatch Loss= 14.6070, Training Accuracy= 0.170\n",
            "Step 399, Minibatch Loss= 12.5923, Training Accuracy= 0.181\n",
            "Step 449, Minibatch Loss= 14.1033, Training Accuracy= 0.181\n",
            "Step 499, Minibatch Loss= 12.5923, Training Accuracy= 0.181\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R-kg9u1e3s7N"
      },
      "source": [
        "## TensorBoard\n",
        "With TensorBoard you can easily monitor the learning process. Unfortunately in Google Colab you cannot directly use TensorBoard, first you have to tunnel it to become  remotely accessible."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D6SdLQpX3wJu"
      },
      "source": [
        "### Setup\n",
        "By running the following commands you can tunnel TensorBoard to the outside world."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9cdJm9TXGdkL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 253
        },
        "outputId": "200a95ef-f9d7-4abe-9e6b-b72aa748c4de"
      },
      "source": [
        "!wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
        "!unzip ngrok-stable-linux-amd64.zip"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-04-07 08:58:31--  https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
            "Resolving bin.equinox.io (bin.equinox.io)... 52.200.123.104, 52.203.66.95, 52.22.145.207, ...\n",
            "Connecting to bin.equinox.io (bin.equinox.io)|52.200.123.104|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 14977695 (14M) [application/octet-stream]\n",
            "Saving to: ‘ngrok-stable-linux-amd64.zip’\n",
            "\n",
            "ngrok-stable-linux- 100%[===================>]  14.28M  14.3MB/s    in 1.0s    \n",
            "\n",
            "2019-04-07 08:58:32 (14.3 MB/s) - ‘ngrok-stable-linux-amd64.zip’ saved [14977695/14977695]\n",
            "\n",
            "Archive:  ngrok-stable-linux-amd64.zip\n",
            "  inflating: ngrok                   \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6u2ZTaQEjwZL"
      },
      "source": [
        "The following will give you an url. By opening it you can access your remote TensorBoard.\n",
        "\n",
        "Modify *LOG_DIR* if needed."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jdjKlI7KGn60",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "c31deea0-e548-40ec-dc28-73da9d07702f"
      },
      "source": [
        "LOG_DIR = './log_test'\n",
        "get_ipython().system_raw(\n",
        "    'tensorboard --logdir {} --host 0.0.0.0 --port 6007 &'\n",
        "    .format(LOG_DIR)\n",
        ")\n",
        "\n",
        "get_ipython().system_raw('./ngrok http 6007 &')\n",
        "\n",
        "!curl -s http://localhost:4040/api/tunnels | python3 -c \\\n",
        "    \"import sys, json; print(json.load(sys.stdin)['tunnels'][0]['public_url'])\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "https://2427574d.ngrok.io\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4tH6AdNoHB2L"
      },
      "source": [
        "### Visualization\n",
        "You can also visualize the computational graph by using TensorBoard. First you have to save the graph to a summary file:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5LC7ni4Xd9IS"
      },
      "source": [
        "writer = tf.summary.FileWriter('./log_test')\n",
        "writer.add_graph(tf.get_default_graph())\n",
        "writer.flush()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R-VN1y76ckKR"
      },
      "source": [
        "Now open the above link and select Graph in the drop-down menu in the top right corner. It will show your computational graph.\n",
        "\n",
        "If you are running this notebook on your own machine, you can just use the following command:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N8GZu0EYHGXO"
      },
      "source": [
        "!tensorboard --logdir ./log_test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2XgTd_-j3wZ6"
      },
      "source": [
        "## High-level API\n",
        "Keras is also available as a high-level API for TensorFlow. You can freely mix Keras layers and your custom layers.\n",
        "\n",
        "Before you move on runtime reset may be necessary because of the visualization. You can do it at: **Runtime -> Reset all runtimes...**. (Warning: You will lose all data from your virtual machine.) After that do not run any previous block except the [remote setup](#scrollTo=9cdJm9TXGdkL)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "18QEHTywHVUN"
      },
      "source": [
        "#### TensorBoard Callback\n",
        "Using **[tf.keras.callbacks.TensorBoard](https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/TensorBoard)** we can easily log information during training, that can be later visualized using TensorBoard. It allows you to visualize dynamic graphs of your training and test metrics, as well as activation histograms for the different layers in your model.\n",
        "\n",
        "**Main arguments:**\n",
        "\n",
        "*  **log_dir:** the path of the directory where to save the log files to be parsed by TensorBoard.\n",
        "* **histogram_freq:** frequency (in epochs) at which to compute activation and weight histograms for the layers of the model. If set to 0, histograms won't be computed. Validation data (or split) must be specified for histogram visualizations.\n",
        "* **batch_size:** size of batch of inputs to feed to the network for histograms computation.\n",
        "* **write_graph:** whether to visualize the graph in TensorBoard. The log file can become quite large when write_graph is set to True.\n",
        "* **write_grads:** whether to visualize gradient histograms in TensorBoard.  histogram_freq must be greater than 0.\n",
        "* **write_images:** whether to write model weights to visualize as image in TensorBoard.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xQ0v5nPL32eJ"
      },
      "source": [
        "from tensorflow.keras.callbacks import TensorBoard\n",
        "\n",
        "batch_size = 32\n",
        "\n",
        "tbCallBack = TensorBoard(log_dir='./log_keras', \n",
        "                         histogram_freq=1,\n",
        "                         batch_size=batch_size,\n",
        "                         write_graph=True,\n",
        "                         write_grads=True,\n",
        "                         write_images=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hVc6CqOIHcUJ"
      },
      "source": [
        "#### Visualization\n",
        "With TensorBoard you can see details about the network during and after training. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QkPjAFFBjZ07",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ee2728e0-6407-4484-92ee-2ec03082e995"
      },
      "source": [
        "LOG_DIR = './log_keras'\n",
        "get_ipython().system_raw(\n",
        "    'tensorboard --logdir {} --host 0.0.0.0 --port 6007 &'\n",
        "    .format(LOG_DIR)\n",
        ")\n",
        "\n",
        "get_ipython().system_raw('./ngrok http 6007 &')\n",
        "\n",
        "!curl -s http://localhost:4040/api/tunnels | python3 -c \\\n",
        "    \"import sys, json; print(json.load(sys.stdin)['tunnels'][0]['public_url'])\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "https://953864d8.ngrok.io\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fNTyY-MQHW4V"
      },
      "source": [
        "#### Build an MLP\n",
        "But for now we have nothing to see, so let's build and train a simple neural network. During training TensorBoard will automatically update."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jUFGSBJeHb4r",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 390
        },
        "outputId": "c9b144a8-47f4-4309-a863-4ff01f8ab0c3"
      },
      "source": [
        "import tensorflow as tf\n",
        "mnist = tf.keras.datasets.mnist\n",
        "\n",
        "(x_train, y_train),(x_test, y_test) = mnist.load_data()\n",
        "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
        "\n",
        "model = tf.keras.models.Sequential([\n",
        "  tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
        "  tf.keras.layers.Dense(128, activation='relu'),\n",
        "  tf.keras.layers.Dropout(0.2),\n",
        "  tf.keras.layers.Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.fit(x_train, y_train, epochs=5, batch_size=batch_size, \n",
        "          verbose=1,\n",
        "          validation_data=(x_test, y_test),\n",
        "          callbacks=[tbCallBack])\n",
        "model.evaluate(x_test, y_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/layers/core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/5\n",
            "60000/60000 [==============================] - 17s 277us/sample - loss: 0.2968 - acc: 0.9129 - val_loss: 0.1430 - val_acc: 0.9577\n",
            "Epoch 2/5\n",
            "60000/60000 [==============================] - 15s 249us/sample - loss: 0.1423 - acc: 0.9577 - val_loss: 0.1050 - val_acc: 0.9672\n",
            "Epoch 3/5\n",
            "60000/60000 [==============================] - 17s 282us/sample - loss: 0.1062 - acc: 0.9678 - val_loss: 0.0848 - val_acc: 0.9744\n",
            "Epoch 4/5\n",
            "60000/60000 [==============================] - 15s 255us/sample - loss: 0.0880 - acc: 0.9726 - val_loss: 0.0750 - val_acc: 0.9764\n",
            "Epoch 5/5\n",
            "60000/60000 [==============================] - 16s 269us/sample - loss: 0.0745 - acc: 0.9761 - val_loss: 0.0766 - val_acc: 0.9766\n",
            "10000/10000 [==============================] - 0s 44us/sample - loss: 0.0766 - acc: 0.9766\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.07656257087504491, 0.9766]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0KbKX_Gu32re"
      },
      "source": [
        "## References\n",
        "* Low-level introduction: https://www.tensorflow.org/guide/low_level_intro\n",
        "* Tutorials: https://www.tensorflow.org/tutorials/\n",
        "* Examples: https://github.com/aymericdamien/TensorFlow-Examples\n",
        "* TensorBoard in Google Colab: https://www.dlology.com/blog/quick-guide-to-run-tensorboard-in-google-colab/\n"
      ]
    }
  ]
}