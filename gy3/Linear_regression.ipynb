{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Linear regression.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2pKobZd0KxwP",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "**Created by:**\n",
        "\n",
        "__[Viktor Varga](https://github.com/vvarga90)__\n",
        "\n",
        "**Translated by:**\n",
        "\n",
        "__Gulyás János Adrián__\n",
        "\n",
        "<br>\n",
        "\n",
        "<img src=\"https://docs.google.com/uc?export=download&id=1WzgXsCoz8O-NeBlJTbuLPC1iIFDmgYt1\" style=\"display:inline-block\">\n",
        "<hr>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UPNGQgO6aXtZ",
        "colab_type": "text"
      },
      "source": [
        "# Linear regression using gradient descent"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EaVq4W9qOIIv",
        "colab_type": "text"
      },
      "source": [
        "Source of the databases at: http://www.dcc.fc.up.pt/~ltorgo/Regression/DataSets.html"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9xF9psXQKgN7",
        "colab_type": "text"
      },
      "source": [
        "Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eugRKr5_Kf7c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import urllib.request\n",
        "\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y_tZc4QSbeev",
        "colab_type": "text"
      },
      "source": [
        "Feature scaling methods: one for calculating the average and the deviation of each feature, which will be used for normalizing the input, and another one that normalizes an input based on the already calculated average and deviation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L2pQ3lkUbhcv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def std_normalization(data):\n",
        "    # params: data: ndarray(n_samples, n_features)\n",
        "    data = np.asarray(data, dtype=np.float64)\n",
        "    eps = 0.0000001\n",
        "    x_mean = np.mean(data, axis=0)\n",
        "    x_std = np.std(data, axis=0)\n",
        "    x_norm = (data-x_mean) / (x_std + eps)\n",
        "    \n",
        "    return x_norm, x_mean, x_std\n",
        "  \n",
        "def std_normalize_with_given_meanstd(data, x_mean, x_std):\n",
        "    eps = 0.0000001\n",
        "    x_norm = (data-x_mean) / (x_std + eps)\n",
        "    return x_norm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b8DWij2kFZRu",
        "colab_type": "text"
      },
      "source": [
        "Loading in the data (\"Wine Quality Data Set\" from the UCI ML repository).\n",
        "\n",
        "**Exercise:** Let's rate the quality of wines on a scale from 1 to 10 based on the wine's chemical composition."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iuxqk43jFPCG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "url = 'https://raw.githubusercontent.com/vvarga90/dlmatek_public_files/master/winequality-red.csv'\n",
        "\n",
        "ftpstream = urllib.request.urlopen(url)\n",
        "content_reg = ftpstream.read().decode('utf-8')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M_XRgr1mJFhs",
        "colab_type": "text"
      },
      "source": [
        "Processing data: from text file to numpy array."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dPMWRt5qHfDy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 112
        },
        "outputId": "3219cc84-b649-4819-cf7c-b8744611d397"
      },
      "source": [
        "lines = content_reg.split('\\n')\n",
        "words = [line.split(';') for line in lines]\n",
        "attr_names = words[0]\n",
        "data = words[1:1001]\n",
        "data = [[float(item) for item in rec] for rec in data]\n",
        "data = np.array(data, dtype=np.float32)\n",
        "\n",
        "features = data[:,:-1]\n",
        "labels = data[:,-1]\n",
        "\n",
        "print(\"Data attributes:\")\n",
        "print(attr_names)\n",
        "print(features.shape)\n",
        "print(labels.shape)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Data attributes:\n",
            "['\"fixed acidity\"', '\"volatile acidity\"', '\"citric acid\"', '\"residual sugar\"', '\"chlorides\"', '\"free sulfur dioxide\"', '\"total sulfur dioxide\"', '\"density\"', '\"pH\"', '\"sulphates\"', '\"alcohol\"', '\"quality\"']\n",
            "(1000, 11)\n",
            "(1000,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1YocW5c_Ku4W",
        "colab_type": "text"
      },
      "source": [
        "Selecting X, Y, separating into a train and a test set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ATDtyb6K-dK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 92
        },
        "outputId": "0c9751b9-42f6-4d2f-be90-f33873128eea"
      },
      "source": [
        "X_train = features[:500]\n",
        "y_train = labels[:500]\n",
        "X_test = features[500:]\n",
        "y_test = labels[500:]\n",
        "\n",
        "print(X_train.shape)\n",
        "print(y_train.shape)\n",
        "print(X_test.shape)\n",
        "print(y_test.shape)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(500, 11)\n",
            "(500,)\n",
            "(500, 11)\n",
            "(500,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "munYcZbIksAL"
      },
      "source": [
        "As a reminder we would like to find the optimal parameter $\\theta$ for the function\n",
        "$$f_{\\theta}(x)=\\theta_0+\\theta x$$\n",
        "such that it minimizes the squared error:\n",
        "$$L(\\mathcal{D},\\theta)=\\frac{1}{N}\\sum_{n=1}^N(y_n-f_{\\theta}(x_n))^2.$$\n",
        "\n",
        "On the lecture we learned about the normal equation to solve this:\n",
        "$$\\theta=(X^TX)^{-1}X^Ty$$\n",
        "Turns out this is a really bad approach to solve this problem because numerical stabilities (see the exercise at the end of the notebook). Instead, we are going to use gradient descent, to solve the problem. First note that (ignoring $\\theta_0$):\n",
        "$$L(\\mathcal{D},\\theta)=\\frac{1}{N}\\sum_{n=1}^N(y_n-f_{\\theta}(x_n))^2=\\frac{1}{N}\\sum_{n=1}^N(y_n-\\theta x_n)^2.$$\n",
        "This is a quadratic function in $\\theta$, we can plot it:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KeTUVn2eWoHM",
        "colab_type": "text"
      },
      "source": [
        "![alt](https://drive.google.com/uc?export=download&id=1kAr_3NqddrbZua1Z5EOgzPnJlQphpKX6)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6DDTpewTWDkJ",
        "colab_type": "text"
      },
      "source": [
        "The algorithm of gradient descent:\n",
        "1. Initialize $\\theta^{(0)}$ to zero (or set randomly)\n",
        "2. Update $\\theta$:  \n",
        "$\\theta^{(t)}=\\theta^{(t-1)}−\\alpha\\nabla_{\\theta} L(\\mathcal D, \\theta)$\n",
        "3. Iterate until it converges, or for a certain number of iterations.\n",
        "\n",
        "In our case $\\nabla_{\\theta} L(\\mathcal D, \\theta)=-2x_n(y_n-\\theta x_n)$. \n",
        "The linear regression class. `fit_intercept` is `True` if we want to use the $\\theta_0$ constant parameter."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "UIe5x8koksAj",
        "colab": {}
      },
      "source": [
        "class LinearRegression:\n",
        "    def __init__(self, lr=0.01, num_iter=10000, fit_intercept=True):\n",
        "        self.lr = lr\n",
        "        self.num_iter = num_iter\n",
        "        self.fit_intercept = fit_intercept\n",
        "        self.eps = 0.00000001\n",
        "    \n",
        "    def __add_intercept(self, X):\n",
        "        # add a column of 1s\n",
        "        intercept = np.ones((X.shape[0], 1))\n",
        "        return np.concatenate((intercept, X), axis=1)\n",
        "    \n",
        "    def __loss(self, h, y):\n",
        "        return np.mean(np.square(h - y))\n",
        "    \n",
        "    def fit(self, X, y):\n",
        "        if self.fit_intercept:\n",
        "            X = self.__add_intercept(X)\n",
        "        \n",
        "        # weights initialization\n",
        "        self.theta = np.zeros(X.shape[1])\n",
        "        \n",
        "        loss_vals = []\n",
        "        for i in range(self.num_iter):\n",
        "            h = np.dot(X, self.theta)\n",
        "            gradient = 2*np.dot(X.T, h - y) / y.size\n",
        "            self.theta -= self.lr * gradient\n",
        "            \n",
        "            h = np.dot(X, self.theta)\n",
        "            loss = self.__loss(h, y)\n",
        "            loss_vals.append(loss)\n",
        "        \n",
        "        return loss_vals\n",
        "    \n",
        "    def predict(self, X):\n",
        "        if self.fit_intercept:\n",
        "            X = self.__add_intercept(X)\n",
        "    \n",
        "        return np.dot(X, self.theta)\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ixJ52jmNJlRX",
        "colab_type": "text"
      },
      "source": [
        "Feature scaling."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Hg6suT-0lwb7",
        "colab": {}
      },
      "source": [
        "X_train_n, x_mean, x_std = std_normalization(X_train)\n",
        "X_test_n = std_normalize_with_given_meanstd(X_test, x_mean, x_std)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "In-I8wdxlwca"
      },
      "source": [
        "Training and evaluation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "9bZ5AIn0lwci",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 302
        },
        "outputId": "b8af71a3-6239-4e1d-9162-a53367677110"
      },
      "source": [
        "# if features are not scaled to same magnitude higher learning rate causes big fluctuations\n",
        "# model = LinearRegression(lr=0.00001, num_iter=50000)   \n",
        "\n",
        "model = LinearRegression(lr=0.0005, num_iter=10000)\n",
        "\n",
        "loss = model.fit(X_train_n, y_train)\n",
        "\n",
        "plt.plot(loss)\n",
        "plt.show();\n",
        "\n",
        "preds_train = model.predict(X_train_n)\n",
        "print(\"Training mean absolute error: \" + str(np.fabs(preds_train - y_train).mean()))\n",
        "preds_test = model.predict(X_test_n)\n",
        "print(\"Test mean absolute error: \" + str(np.fabs(preds_test - y_test).mean()))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Training mean absolute error: 0.5001925998307676\n",
            "Test mean absolute error: 0.5147913228627844\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "36dNt0CjMWN9",
        "colab_type": "text"
      },
      "source": [
        "**Conclusion:** The trained model could predict the wines' quality rating on a scale from 1 to 10 with an absolute error of around 0.5. Considering that most of the wines in the database have a rating between 3 and 8, this is not exactly a remarkable result. Perhaps the wine that's proclaimed to be high quality is the same as a cheaper variant, and calling either wine higher quality is just plain snobbism? Or perhaps a linear function based on the chemical compound is not enough to approximate the quality sticker given by professionals, and we require a more complex model?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-JDt0MGaItgs",
        "colab_type": "text"
      },
      "source": [
        "Linear regression is already implemented in multiple python libraries. One implementation from the sklearn package:\n",
        "https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_o-_D1coZneR",
        "colab_type": "text"
      },
      "source": [
        "## Exercise\n",
        "\n",
        "Solve the linear regression problem with the normal equation (see above). Compare it to our gradient descent solution:  \n",
        "- Which one has the lower train/test error?\n",
        "- Which one has the lower loss (the $L(\\mathcal D,\\theta)$ value)?\n",
        "\n",
        "*(For those who did numerical analysis)* We are inverting the matrix $X^TX$, which can be numerically unstable. Calculate the condition number of $X^TX$. What does it mean for the stability of our normal equation? Recall that\n",
        "$$\\kappa(M)=\\frac{\\sigma_{max}(M)}{\\sigma_{min}(M)},$$\n",
        "where $\\sigma_{max}(M)$ and $\\sigma_{max}(M)$ are the maximal and minimal singular values of $M$.\n"
      ]
    }
  ]
}